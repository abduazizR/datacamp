---
title: "Importing Data in R"
author: "Abdullah Abdelaziz"
date: "4/18/2020"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    df_print: paged
    toc: true
    toc_float: false
    toc_depth: 4
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load packages
```{r Packages, message=FALSE, warning=FALSE}
library(tidyr)
library(tidyverse)
library(forcats)
library(readr)
library(skimr)
library(data.table)
library(readxl)
library(gdata)
# for SAS STATA SPSS files
library(haven) #or
library(foreign)
```

# Reading `csv` files
## example
```{r}
# Import swimming_pools.csv: pools
pools <- read_csv("swimming_pools.csv")

# Print the structure of pools
skim(pools)
```

# Importing tab-delimited or other data with different separators

## Example of a tab-delimited
```{r}
# Import hotdogs.txt: hotdogs
hotdogs <- read.delim("hotdogs.txt", header = FALSE, stringsAsFactors = FALSE)

# Summarize hotdogs
summary(hotdogs)
```

## Dealing with tabular files with different types of separators

```{r}
# Import hotdogs but this time using read_table function
hotdogs <- read.table("hotdogs.txt",sep = "\t", col.names = c("type","calories","sodium"))

# Call head() on hotdogs
head(hotdogs)
```

## Making use of arguments

```{r}
# Select the hot dog with the least calories: lily
lily <- hotdogs %>%
  filter(calories == min(calories))

# Select the observation with the most sodium: tom
tom <- hotdogs %>% 
  filter(sodium == max(sodium))

# print lily and tom
lily
tom
```

### Alternative method to do the task above
```{r}
# Select the hot dog with the least calories: lily
lily <- hotdogs[which.min(hotdogs$calories), ]

# Select the observation with the most sodium: tom
tom <- hotdogs[which.max(hotdogs$sodium),]

# Print lily and tom
lily
tom

```


## Column classes\types
```{r}
# Edit the col_types argument to import the data correctly: hotdogs2

hotdogs2 <- read.delim("hotdogs.txt", header = FALSE, col.names = c("type","calories","sodium"),colClasses = c("factor","NULL","numeric"))

# examine
head(hotdogs2)
```

**Note**: If a column is set to `"NULL"` in the `colClasses` vector, this column will be skipped and will not be loaded into the data frame.

----

# Using `readr` package

## import `potatoes.csv`
```{r}
# Import potatoes.csv with read_csv(): potatoes
potatoes <- read_csv("potatoes.csv")
```

## using `read_tsv()`
- Use `read_tsv()` to import the potatoes data from `potatoes.txt` and store it in the data frame `potatoes`. In addition to the path to the file, you'll also have to specify the `col_names` argument; you can use the properties vector for this.
- Call head() on potatoes to show the first observations of your dataset.

```{r}
# set columns names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")
#import data
potatoes <- read_tsv("potatoes.txt", col_names = properties)

# show first observations
head(potatoes)
```

## `read_delim` function
- Import all the data in `"potatoes.txt"` using `read_delim()`; store the resulting data frame in `potatoes`
- Print out `potatoes`.

```{r}
# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")
potatoes <- read_delim("potatoes.txt", delim = "\t", col_names = properties)

# examine
potatoes
```

## call to import observations 7, 8, 9, 10 and 11 from potatoes.txt: `potatoes_fragment`

```{r}
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")
potatoes_fragment <- read_delim("potatoes.txt", delim = "\t", col_names = properties, skip = 6, n_max = 5)

#examine
potatoes_fragment
```

## `col_type` argument
You can also specify which types the columns in your imported data frame should have. You can do this with `col_types`. If set to `NULL`, the default, functions from the readr package will try to find the correct types themselves. You can manually set the types with a string, where each character denotes the class of the column: `c`haracter, `d`ouble, `i`nteger and `l`ogical. `_` skips the column as a whole.

**Instructions:**
- In the second `read_tsv()` call, edit the `col_types` argument to import all columns as characters (`c`). Store the resulting data frame in `potatoes_char`.
- Print out the structure of `potatoes_char` and verify whether all column types are `chr`, short for `character`.

```{r}
potatoes_char <- read_tsv("potatoes.txt",col_types = "cccccccc",col_names = properties)

# print structures of potatoes_char
str(potatoes_char)
```


## col_types with collectors

Another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a `list()` to the `col_types` argument of `read_` functions to tell them how to interpret values in a column.

For a complete list of collector functions, you can take a look at the `collector` documentation. For this exercise you will need two collector functions:
  - `col_integer()`: the column should be interpreted as an integer.
  - `col_factor(levels, ordered = FALSE)`: the column should be interpreted as a factor with `levels.`

In this exercise, you will work with `hotdogs.txt`, which is a tab-delimited file without column names in the first row.

**Instructions:**

- `hotdogs` is created for you without setting the column types. Inspect its summary using the `summary()` function.
- Two `collector` functions are defined for you: `fac` and `int`. Have a look at them, do you understand what they're collecting?
- In the second `read_tsv()` call, edit the `col_types` argument: Pass a `list()` with the elements `fac`, `int` and `int`, so the first column is imported as a factor, and the second and third column as integers.
- Create a `summary()` of `hotdogs_factor`. Compare this to the summary of hotdogs.

```{r}
# Import without col_types
hotdogs <- read_tsv("hotdogs.txt", col_names = c("type", "calories", "sodium"))

# Display the summary of hotdogs
summary(hotdogs)

# The collectors you will need to import the data
fct <- col_factor(levels = c("Beef","Meat","Poultry"))
int <- col_integer()

# Edit the col_types argument to import the data correctly: hotdogs_factor
hotdogs_factor <- read_tsv("hotdogs.txt", col_names = c("type", "calories", "sodium"),col_types = list(fct,int,int))

# Display the summary of hotdogs_factor
summary(hotdogs_factor)
```

## `fread()` package
- You still remember how to use `read.table()`, right? Well, `fread()` is a function that does the same job with very similar arguments. It is extremely easy to use and blazingly fast! Often, simply specifying the path to the file is enough to successfully import your data.

- Don't take our word for it, try it yourself! You'll be working with the `potatoes.csv` file, that's available in your workspace. Fields are delimited by commas, and the first line contains the column names.

**Instructions**:
  - Import `"potatoes.csv"` with `fread()`. Simply pass it the file path and see if it worked. Store the result in a variable potatoes.
  - Print out `potatoes`.

```{r}
potatoes <- fread("potatoes.csv")

# print
potatoes
```

```{r}
# Import columns 6 and 8 of potatoes.csv: potatoes
potatoes <- fread("potatoes.csv", select = c(6,8))

# Plot texture (x) and moistness (y) of potatoes
plot(potatoes$texture,potatoes$moistness)
```

# Read Excel files
## How to know the number of excel sheets in your excel file
```{r}
excel_sheets("urbanpop.xlsx")
```

## How to import an excel file
```{r}
read_excel("urbanpop.xlsx")
```

By using the `read_excel` function, you will only get the first sheet of your file. In case you want to specify a specific sheet by number or name, do the following

```{r}
# read the first sheet of urbanpop.xlsx
read_excel("urbanpop.xlsx", sheet = 1)
```

- You can also call sheets by name (You can know the sheets' names using `excel_sheets()` function)
```{r}
# read the second sheet
read_excel("urbanpop.xlsx", sheet = "1967-1974")
```

- Let's try combining our sheets into a list
```{r}
# Read the sheets and assign them to variables, one by one
pop_1 <- read_excel("urbanpop.xlsx", sheet = 1)
pop_2 <- read_excel("urbanpop.xlsx", sheet = 2)
pop_3 <- read_excel("urbanpop.xlsx", sheet = 3)

# Put pop_1, pop_2 and pop_3 in a list: pop_list
pop_list <- list(pop_1, pop_2, pop_3)

# Display the structure of pop_list
str(pop_list)
```

An Alternative way to read all sheets at once is by using the `lapply()` function as follow

```{r}
# use lapply to read all Excel sheets and save them to pop_list
pop_list <- lapply(excel_sheets("urbanpop.xlsx"), read_excel, path="urbanpop.xlsx")

# Display the structure of pop_list
str(pop_list)
```

- We got the same results!!

----

## More on reading excel files
### The col_names argument

Apart from `path` and `sheet`, there are several other arguments you can specify in `read_excel()`. One of these arguments is called `col_names`.

By default it is `TRUE`, denoting whether the first row in the Excel sheets contains the column names. If this is not the case, you can set `col_names` to `FALSE.` In this case, R will choose column names for you. You can also choose to set `col_names` to a character vector with names for each column. It works exactly the same as in the `readr` package.

You'll be working with the `urbanpop_nonames.xlsx` file. It contains the same data as `urbanpop.xlsx` but has no column names in the first row of the excel sheets.

**Instructions**
- Import the first Excel sheet of `urbanpop_nonames.xlsx` and store the result in `pop_a.` Have R set the column names of the resulting data frame itself.
- Import the first Excel sheet of `urbanpop_nonames.xlsx`; this time, use the `cols` vector that has already been preparedfor you to specify the column names. Store the resulting data frame in `pop_b`.
- Print out the summary of `pop_a`.
- Print out the summary of `pop_b`. Can you spot the difference with the other summary?

```{r}
# Import the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a
pop_a <- read_excel("urbanpop_nonames.xlsx", sheet = 1,col_names = FALSE)

# Import the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b
cols <- c("country", paste0("year_", 1960:1966))
pop_b <- read_excel("urbanpop_nonames.xlsx", sheet = 1, col_names = cols)

# Print the summary of pop_a
summary(pop_a)

# Print the summary of pop_b
summary(pop_b)
```

### The skip argument
Another argument that can be very useful when reading in Excel files that are less tidy, is `skip`. With `skip`, you can tell R to ignore a specified number of rows inside the Excel sheets you're trying to pull data from. Have a look at this example:

`read_excel("data.xlsx", skip = 15)`

In this case, the first 15 rows in the first sheet of `"data.xlsx"` are ignored.

If the first row of this sheet contained the column names, this information will also be ignored by `readxl`. Make sure to set col_names to FALSE or manually specify column names in this case!

The file `urbanpop.xlsx` is available in your directory; it has column names in the first rows.

**Instructions**
- Import the second sheet of `"urbanpop.xlsx"`, but skip the first 21 rows. Make sure to set `col_names = FALSE`. Store the resulting data frame in a variable `urbanpop_sel`.
- Select the first observation from `urbanpop_sel` and print it out.

```{r}
# Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel
urbanpop_sel <- read_excel("urbanpop.xlsx",sheet = 2, skip = 21, col_names = FALSE)

# Print out the first observation from urbanpop_sel
head(urbanpop_sel,1)

```


---

# Reproducible Excel work with XLConnect

##Connect to a workbook

- When working with `XLConnect`, the first step will be to load a workbook in your R session with `loadWorkbook()`; this function will build a "bridge" between your Excel file and your R session.

- In this and the following exercises, you will continue to work with `urbanpop.xlsx`, containing urban population data throughout time. The Excel file is available in your current working directory.

**Instructions**

1- Load the `XLConnect` package using `library()`; it is already installed on DataCamp's servers.
2- Use `loadWorkbook()` to build a connection to the `"urbanpop.xlsx"` file in R. Call the workbook `my_book`.
3- Print out the class of `my_book`. What does this tell you?

```{r}
# load XLConnect package
library(XLConnect)

# create my_book to connect your excel file with R
my_book <- loadWorkbook("urbanpop.xlsx")

# check the class of my_book
class(my_book)
```

## List and read Excel sheets

- Just as `readxl` and `gdata`, you can use `XLConnect` to import data from Excel file into R.

- To list the sheets in an Excel file, use `getSheets()`. To actually import data from a sheet, you can use `readWorksheet()`. Both functions require an `XLConnect` workbook object as the first argument.

You'll again be working with `urbanpop.xlsx`. The `my_book` object that links to this Excel file has already been created.

```{r}
# List the sheets in my_book
getSheets(my_book)

# Import the second sheet in my_book
readWorksheet(my_book, sheet = 2)
```

- How to get a complete overview of an excel file in R
```{r}
# This is an example
my_book <- loadWorkbook("urbanpop.xlsx")
sheets <- getSheets(my_book)
all <- lapply(sheets, readWorksheet, object = my_book)
str(all)
```

- Suppose we're only interested in urban population data of the years 1968, 1969 and 1970. The data for these years is in the columns 3, 4, and 5 of the second sheet. Only selecting these columns will leave us in the dark about the actual countries the figures belong to.

```{r}
# Import columns 3, 4, and 5 from second sheet in my_book: urbanpop_sel
urbanpop_sel <- readWorksheet(my_book, sheet = 2, startCol =3, endCol =5)

# check
head(urbanpop_sel)
```

- `urbanpop_sel` no longer contains information about the countries now. Can you write another `readWorksheet()` command that imports only the first column from the second sheet? Store the resulting data frame as `countries`.

```{r}
# Import first column from second sheet in my_book: countries
countries <- readWorksheet(my_book, sheet = 2, startCol = 1,endCol = 1)

# Check 
head(countries)
```

Use `cbind()` to paste together `countries` and `urbanpop_sel`, in this order. Store the result as `selection.`
```{r}
# cbind() urbanpop_sel and countries together: selection
selection <- cbind(countries,urbanpop_sel)

# examine
head(selection)
```

## Adapting sheets: Add worksheet

Where `readxl` and `gdata` were only able to import Excel data, `XLConnect`'s approach of providing an actual interface to an Excel file makes it able to edit your Excel files from inside R. In this exercise, you'll create a new sheet. In the next exercise, you'll populate the sheet with data, and save the results in a new Excel file.

You'll continue to work with `urbanpop.xlsx`. The `my_book` object that links to this Excel file is already available.

Where `readxl` and `gdata` were only able to import Excel data, `XLConnect`'s approach of providing an actual interface to an Excel file makes it able to edit your Excel files from inside R. In this exercise, you'll create a new sheet. In the next exercise, you'll populate the sheet with data, and save the results in a new Excel file.

You'll continue to work with `urbanpop.xlsx`. The `my_book` object that links to this Excel file is already available.

**Instructions**
- Use `createSheet()`, to create a new sheet in `my_book`, named `"data_summary"`.
- Use `getSheets()` to verify that `my_book` now represents an Excel file with four sheets.

```{r}
#add the data_summary sheet
createSheet(my_book, "data_summary")

#verfiy
getSheets(my_book)

```

## Populate worksheet

The first step of creating a sheet is done; let's populate it with some data now! `summ`, a data frame with some summary statistics on the two Excel sheets is already coded so you can take it from there.

**Instructions**

```{r}
# XLConnect is already available

# Build connection to urbanpop.xlsx
my_book <- loadWorkbook("urbanpop.xlsx")

# Add a worksheet to my_book, named "data_summary"
createSheet(my_book, "data_summary")

# Create data frame: summ
sheets <- getSheets(my_book)[1:3]
dims <- sapply(sheets, function(x) dim(readWorksheet(my_book, sheet = x)), USE.NAMES = FALSE)
summ <- data.frame(sheets = sheets,
                   nrows = dims[1, ],
                   ncols = dims[2, ])

# Add data in summ to "data_summary" sheet
writeWorksheet(my_book, summ, "data_summary")

# Save workbook as summary.xlsx
saveWorkbook(my_book, "summary.xlsx")
```

## Renaming sheets

Come to think of it, `"data_summary"` is not an ideal name. As the summary of these excel sheets is always data-related, you simply want to name the sheet `"summary"`.

The workspace already contains a workbook, `my_book`, that refers to an Excel file with 4 sheets: the three data sheets, and the `"data_summary"` sheet.

**Instructions**
- Use `renameSheet()` to rename the fourth sheet to `"summary"`.
- Next, call `getSheets()` on `my_book` to print out the sheet names.
- Finally, make sure to actually save the `my_book` object to a new Excel file, `"renamed.xlsx"`.

```{r}
# Rename "data_summary" sheet to "summary"
renameSheet(my_book, "data_summary", "summary")

# Print out sheets of my_book
getSheets(my_book)

# Save workbook to "renamed.xlsx"
saveWorkbook(my_book, "renamed.xlsx")
```

## Removing sheets

After presenting the new Excel sheet to your peers, it appears not everybody is a big fan. Why summarize sheets and store the info in Excel if all the information is implicitly available? To hell with it, just remove the entire fourth sheet!

**Instructions**

- Load the `XLConnect` package.
- Build a connection to `"renamed.xlsx"`, the Excel file that you've built in the previous exercise; it's available in your working directory. Store this connection as `my_book`.
- Use `removeSheet()` to remove the fourth sheet from `my_book`. The sheet name is `"summary"`. Recall that `removeSheet()` accepts either the index or the name of the sheet as the second argument.
- Save the resulting workbook, `my_book`, to a file `"clean.xlsx".`


```{r}
# Load package
#done

# build connection
my_book <- loadWorkbook("renamed.xlsx")

# remove the fourth sheet "summary
removeSheet(my_book, "summary")

# Save your workbook as clean.xlsx
saveWorkbook(my_book, "clean.xlsx")
```

---

# Importing data from databases (Part 1)
## Connect to a database (SQL database)
- To import data from a SQL database, you should connect to it.
- You need different packages depending on the database you want to connect to. All of these packages do this in a uniform way, as specified in the `DBI` package.

- `dbConnect()` creates a connection between your R session and a SQL database. The first argument has to be a `DBIdriver` object, that specifies how connections are made and how data is mapped between R and the database. Specifically for MySQL databases, you can build such a driver with `RMySQL::MySQL().` 

- If the MySQL database is a remote database hosted on a server, you'll also have to specify the following arguments in `dbConnect()`: `dbname`, `host`, `port`, `user` and `password`. Most of these details have already been provided.

```{r}
# Load the DBI package
library(DBI)

# Make a connection 
con <- dbConnect(RMySQL::MySQL(), 
                 dbname = "tweater", 
                 host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com", 
                 port = 3306,
                 user = "student",
                 password = "datacamp")

# Check the type of con
con
```

- Now we are connected to the database.
- Let's move to explore the database in hands

## List the database tables
```{r}
# List the database tables and save it into a vector: tabls
tables <- dbListTables(con)

#Display the structure of tables
tables
```

- So, we have three tables in the database

## Importing a specific table from a database
```{r}
# Import the users table from tweater
users <- dbReadTable(con, "users")

# Print users
users
```

## Importing all tables in a database
Next to the `users`, we're also interested in the `tweats` and `comments` tables. However, separate `dbReadTable()` calls for each and every one of the tables in your database would mean a lot of code duplication. Remember about the `lapply()` function? You can use it again here! 

```{r}
# Create a vector for table names
table_names <- dbListTables(con)

# Import all tables
tables <- lapply(table_names,dbReadTable,conn = con)

# Print out tables
tables
```

- Since this is a relational database, you can examine how do the tables relate?

## Disconnecting from a database
```{r}
dbDisconnect(con)
```

- Let's check if that worked out
```{r}
#dbListResults()
#To have the html file I had to make it in comment format
```

- This error means that we've successfully disconnected from the database

# SQL Queries from inside R
## Query tweater (1)

- In your life as a data scientist, you'll often be working with huge databases that contain tables with millions of rows. If you want to do some analyses on this data, it's possible that you only need a fraction of this data. In this case, it's a good idea to send SQL queries to your database, and only import the data you actually need into R.

`dbGetQuery()` is what you need. As usual, you first pass the connection object to it. The second argument is an SQL query in the form of a character string. This example selects the `age` variable from the `people` dataset where `gender` equals `"male"`:

`dbGetQuery(con, "SELECT age FROM people WHERE gender = 'male'")`

**Instructions**
- Use `dbGetQuery()` to create a data frame, `elisabeth`, that selects the `tweat_id` column from the `comments` table where `elisabeth` is the commenter, her user_id is 1
- Print out `elisabeth` so you can see if you queried the database correctly.

```{r}
# Let's reconnect to the database as we did before
library(DBI)
con <- dbConnect(RMySQL::MySQL(),
                 dbname = "tweater",
                 host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com",
                 port = 3306,
                 user = "student",
                 password = "datacamp")

# Import tweat_id column from comments table where user_id is 1: elisabeth
elisabeth <- dbGetQuery(con, "
           SELECT tweat_id 
           FROM comments
           WHERE user_id = 1
           ")

# Print elisabeth
elisabeth
```


## Query tweater (2)

- Apart from checking equality, you can also check for less than and greater than relationships, with `<` and `>`, just like in R.

**Instructions**
- Create a data frame, `latest`, that selects the `post` column from the `tweats` table observations where the `date` is higher than `'2015-09-21'`.
- Print out `latest.`

```{r}
# We can write our query immediately since we're already connected to the database
latest  <- dbGetQuery(con,
                      "SELECT post
                      FROM tweats
                      WHERE date > '2015-09-21'
                      ")

# Print latest
latest

```

**Instructions**
- Create an R data frame, `specific`, that selects the `message` column from the `comments` table where the `tweat_id` is 77 and the `user_id` is greater than 4.
- Print `specific.`

```{r}
# Create a dataframe : specific
specific <- dbGetQuery(con,"
                       SELECT message
                       FROM comments
                       WHERE tweat_id = 77 AND user_id > 4
                       ")

# Print specific
specific
```

**Instructions**

- Create a data frame, `short`, that selects the `id` and `name` columns from the `users` table where the number of characters in the `name` is strictly less than 5.
- Print `short`.

```{r}
# create short
short <- dbGetQuery(con,
                    "SELECT id, name
                    FROM users
                    WHERE CHAR_LENGTH(name) < 5")

# Print short
short
```

## Send - Fetch - Clear
You've used `dbGetQuery()` multiple times now. This is a virtual function from the DBI package, but is actually implemented by the `RMySQL` package. Behind the scenes, the following steps are performed:

    Sending the specified query with `dbSendQuery()`;
    Fetching the result of executing the query on the database with `dbFetch()`;
    Clearing the result with `dbClearResult()`.

Let's not use `dbGetQuery()` this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query's result in chunks rather than all at once. You can do this by specifying the `n` argument inside `dbFetch()`.

**Instructions**

- Use `dbSendQuery()` to select all columns in the `comments` table for the users with an id above 4.
-Use `dbFetch()` twice. In the first call, import only two records of the query result by setting the `n` argument to 2. In the second call, import all remaining queries (don't specify `n`). In both calls, simply print the resulting data frames.
- Clear `res` with `dbClearResult()`.

```{r}
# Send query to the database
res <- dbSendQuery(con, "
                   SELECT*
                   FROM comments
                   WHERE user_id > 4")

# Use dbFetch() twice
dbFetch(res, n =2)
dbFetch(res)

# Clear res
dbClearResult(res)
```

## Be polite and get the hell out of the database

Every time you connect to a database using dbConnect(), you're creating a new connection to the database you're referencing. RMySQL automatically specifies a maximum of open connections and closes some of the connections for you, but still: it's always polite to manually disconnect from the database afterwards. You do this with the dbDisconnect() function.

**Instructions**

    Using the technique you prefer, build a data frame long_tweats. It selects the post and date columns from the observations in tweats where the character length of the post variable exceeds 40.
    Print long_tweats.
    Disconnect from the database by using dbDisconnect().

```{r}
# Create long_tweats
long_tweats <- dbGetQuery(con, "
                          SELECT post, date
                          FROM tweats
                          WHERE CHAR_LENGTH(post) > 40
                          ")

# Print long_tweats
print(long_tweats)
```

```{r}
# Disconnect from the database
dbDisconnect(con)
```

# Importing Data from Web

## `csv` files
**Instructions**

    Use url_csv to read in the .csv file it is pointing to. Use the read_csv() function. The .csv contains column names in the first row. Save the resulting data frame as pools.


```{r}
# Save the dataset link to url_csv
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"

# Import the csv file: pools
pools <- read_csv(url_csv)

# Print pools
pools
```

## `tsv` or `txt` files
  - Similarly, use url_delim to read in the online .txt file. Use the read_tsv() function and store the result as potatoes.
  
```{r}
# Do the same as above
url_delim <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt"

# Import data : potatoes
potatoes <- read_tsv(url_delim)

# Print potatoes
potatoes
```


## Downloading and importing excel files 
This approach is useful when we cannot import our data directly to R. We will have to download them first then import them.

When you learned about `gdata`, it was already mentioned that `gdata` can handle `.xls` files that are on the internet. `readxl` can't, at least not yet. The URL with which you'll be working is already available in the sample code. You will import it once using `gdata` and once with the `readxl` package via a workaround.

```{r}
# Make sure that readxl and gdata are loaded

# specify your url: url_xls
#url_xls <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls"

# Import the file using gdata package: excel_gdata
#excel_gdata <- read.xls(url_xls)

# Print
#excel_gdata
```

- For some reason, it didn't work out
- Let's try to import the xls file using the read_excel function
```{r}
# We need to download the file from the url 

# Set your url
#url_xls <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls"

# We then need to specify our path
#dest_path <- file.path(getwd(), "local_latitude.xls")

# download
#download.file(url_xls, dest_path)

# let's import it to R: 
#excel_readxl <- read_excel("local_latitude.xls")

# Examine 
#excel_readxl
```

- From some reason I can't download this file here, but it works perfectly fine on Rstudio cloud

## Downloading any file, secure or not

In the previous exercise you've seen how you can read excel files on the web using the `read_excel` package by first downloading the file with the `download.file()` function.

There's more: with `download.file()` you can download any kind of file from the web, using HTTP and HTTPS: images, executable files, but also `.RData` files. An `RData` file is very efficient format to store R data.

You can load data from an `RData` file using the `load()` function, but this function does not accept a URL string as an argument. In this exercise, you'll first download the RData file securely, and then import the local data file.

```{r}
# https URL to the wine RData file.
url_rdata <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData"

# Download the wine file to your working directory
dest_path <- file.path(getwd(), "wine_local.RData")
download.file(url_rdata,dest_path)

# Load the wine data into your workspace using load()
load("wine_local.RData")

# Print out the summary of the wine data
summary(wine)
```

## HTTP? httr! (1)

Downloading a file from the Internet means sending a GET request and receiving the file you asked for. Internally, all the previously discussed functions use a GET request to download files.

`httr` provides a convenient function, `GET()`1 to execute this GET request. The result is a `response` object, that provides easy access to the status code, content-type and, of course, the actual content.

You can extract the content from the request using the `content()` function. At the time of writing, there are three ways to retrieve this content: as a raw object, as a character vector, or an R object, such as a list. If you don't tell `content()` how to retrieve the content through the `as` argument, it'll try its best to figure out which type is most appropriate based on the content-type.

**Instructions**
- Load the `httr` package. It's already installed on DataCamp's servers.
- Use `GET()` to get the URL stored in `url`. Store the result of this `GET()` call as resp.
- Print the `resp` object. What information does it contain?
- Get the content of resp using content() and set the as argument to "raw". Assign the resulting vector to raw_content.
- Print the first values in raw_content with head().

```{r}
# Load the httr package
library(httr)

# Get the url, save response to resp
url <- "http://www.example.com/"
resp <- GET(url)

# Print resp
print(resp)

# Get the raw content of resp: raw_content
raw_content <- content(resp, as = "raw")

# Print the head of raw_content
head(raw_content)
```

## HTTP? httr! (2)

Web content does not limit itself to HTML pages and files stored on remote servers such as DataCamp's Amazon S3 instances. There are many other data formats out there. A very common one is JSON. This format is very often used by so-called Web APIs, interfaces to web servers with which you as a client can communicate to get or store information in more complicated ways.

You'll learn about Web APIs and JSON in the video and exercises that follow, but some experimentation never hurts, does it?

**Instructions**

- Use `GET()` to get the url that has already been specified in the sample code. Store the response as `resp`.
- Print `resp`. What is the content-type?
- Use `content()` to get the content of `resp`. Set the `as` argument to `"text"`. Simply print out the result. What do you see?
- Use `content()` to get the content of resp, but this time do not specify a second argument. R figures out automatically that you're dealing with a JSON, and converts the JSON to a named R list.

```{r}
# httr is already loaded

# Get the url
url <- "http://www.omdbapi.com/?apikey=72bc447a&t=Annie+Hall&y=&plot=short&r=json"
resp <- GET(url)

# Print resp
print(resp)

# Print content of resp as text
content(resp, as = "text")

# Print content of resp
content(resp)
```


# APIs & JSON
## From JSON to R

- In the simplest setting, `fromJSON()` can convert character strings that represent JSON data into a nicely structured R list. Give it a try!

**Instructions**
- Load the `jsonlite` package. 
- `wine_json` represents a JSON. Use `fromJSON()` to convert it to a list, named `wine`.
- Display the structure of `wine`

```{r}
# load json package
library(jsonlite)

# wine_json is a JSON
wine_json <- '{"name":"Chateau Migraine", "year":1997, "alcohol_pct":12.4, "color":"red", "awarded":false}'

# Convert wine_json to a list: wine
wine <- fromJSON(wine_json)

# display the structure of wine
str(wine)

```

## Quandl API

As Filip showed in the video, `fromJSON()` also works if you pass a URL as a character string or the path to a local file that contains JSON data. Let's try this out on the Quandl API, where you can fetch all sorts of financial and economical data.

**Instructions**

- `quandl_url` represents a URL. Use `fromJSON()` directly on this URL and store the result in `quandl_data`.
- Display the structure of `quandl_data`.

```{r}
# jsonlite is preloaded

# Definition of quandl_url
quandl_url <- "https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?auth_token=i83asDsiWUUyfoypkgMz"

# Import Quandl data: quandl_data
quandl_data <- fromJSON(quandl_url)

# Print structure of quandl_data
str(quandl_data)
```

## OMDb API

In the video, you saw how easy it is to interact with an API once you know how to formulate requests. You also saw how to fetch all information on Rain Man from OMDb. Simply perform a GET() call, and next ask for the contents with the content() function. This content() function, which is part of the httr package, uses jsonlite behind the scenes to import the JSON data into R.

However, by now you also know that jsonlite can handle URLs itself. Simply passing the request URL to fromJSON() will get your data into R. In this exercise, you will be using this technique to compare the release year of two movies in the Open Movie Database.

```{r}
# The package jsonlite is already loaded

# Definition of the URLs
url_sw4 <- "http://www.omdbapi.com/?apikey=72bc447a&i=tt0076759&r=json"
url_sw3 <- "http://www.omdbapi.com/?apikey=72bc447a&i=tt0121766&r=json"

# Import two URLs with fromJSON(): sw4 and sw3
sw4 <- fromJSON(url_sw4)
sw3 <- fromJSON(url_sw3)

# Print out the Title element of both lists
sw3$Title
sw4$Title


# Is the release year of sw4 later than sw3?
sw4$Year > sw3$Year 
```

# JSON and jsonlite
## JSON practice (1)

JSON is built on two structures: objects and arrays. To help you experiment with these, two JSON strings are included in the sample code. It's up to you to change them appropriately and then call jsonlite's fromJSON() function on them each time.

**Instructions**
- Change the assignment of `json1` such that the R vector after conversion contains the numbers 1 up to 6, in ascending order. Next, call `fromJSON()` on `json1`.
- Adapt the code for `json2` such that it's converted to a named list with two elements: `a`, containing the numbers 1, 2 and 3 and `b`, containing the numbers 4, 5 and 6. Next, call `fromJSON()` on `json2`.

```{r}
# jsonlite is already loaded

# Challenge 1
json1 <- '[1, 2, 3, 4, 5, 6]'
fromJSON(json1)

# Challenge 2
json2 <- '{"a": [1, 2, 3], "b":[4, 5, 6]}'
fromJSON(json2)
```

## JSON practice (2)

We prepared two more JSON strings in the sample code. Can you change them and call jsonlite's fromJSON() function on them, similar to the previous exercise?

```{r}
# jsonlite is already loaded

# Challenge 1
json1 <- '[[1, 2], [3, 4]]'
fromJSON(json1)

# Challenge 2
json2 <- '[{"a": 1, "b": 2}, {"a": 3, "b": 4}, {"a":5, "b":6}]'
fromJSON(json2)
```

## toJSON()

Apart from converting JSON to R with `fromJSON()`, you can also use `toJSON()` to convert R data to a JSON format. In its most basic use, you simply pass this function an R object to convert to a JSON. The result is an R object of the class `json`, which is basically a character string representing that JSON.

For this exercise, you will be working with a .csv file containing information on the amount of desalinated water that is produced around the world. As you'll see, it contains a lot of missing values. This data can be found on the URL that is specified in the sample code.

```{r}
# jsonlite is already loaded

# URL pointing to the .csv file
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv"

# Import the .csv file located at url_csv: water
water <-read.csv(url_csv, stringsAsFactors = FALSE)

# Convert the data file according to the requirements: water_json
water_json <- toJSON(water)

# Print out water_json
water_json
```

# Importing data from statistical software packages
## Import SAS data with haven

`haven` is an extremely easy-to-use package to import data from three software packages: SAS, STATA and SPSS. Depending on the software, you use different functions:

- SAS: `read_sas()`
- STATA: `read_dta()` (or `read_stata()`, which are identical)
- SPSS: `read_sav()` or `read_por()`, depending on the file type.

All these functions take one key argument: the path to your local file. In fact, you can even pass a URL; `haven` will then automatically download the file for you before importing it.

You'll be working with data on the age, gender, income, and purchase level (0 = low, 1 = high) of 36 individuals (Source: SAS). The information is stored in a SAS file, `sales.sas7bdat`, which is available in your current working directory.

```{r}
# Load the haven package
# the package is already loaded

# Import sales.sas7bdat: sales
sales <- read_sas("sales.sas7bdat")

# Display the structure of sales
str(sales)
```

## Import STATA data with haven

Next up are STATA data files; you can use `read_dta()` for these.

When inspecting the result of the `read_dta()` call, you will notice that one column will be imported as a `labelled` vector, an R equivalent for the common data structure in other statistical environments. In order to effectively continue working on the data in R, it's best to change this data into a standard R class. To convert a variable of the class labelled to a factor, you'll need haven's `as_factor()` function.

In this exercise, you will work with `trade.dta` data on yearly import and export numbers of sugar, both in USD and in weight.

```{r}
# haven is already loaded

# Import the data from the URL: sugar
sugar <- read_dta("http://assets.datacamp.com/production/course_1478/datasets/trade.dta")

# Structure of sugar
str(sugar)

# Convert values in Date column to dates
sugar$Date <- as.Date(as_factor(sugar$Date))

# Structure of sugar again
str(sugar)
```

## Import SPSS data with haven

The `haven` package can also import data files from SPSS. Again, importing the data is pretty straightforward. Depending on the SPSS data file you're working with, you'll need either `read_sav()` - for .sav files - or `read_por()` - for .por files.

In this exercise, you will work with data on four of the Big Five personality traits for 434 persons (Source: University of Bath). The Big Five is a psychological concept including, originally, five dimensions of personality to classify human personality. The SPSS dataset is called `person.sav` and is available in your working directory.

```{r}
# haven is already loaded

# Import person.sav: traits
traits <- read_sav("person.sav")

# Summarize traits
skim(traits)

# Print out a subset of those individuals that scored high on Extroversion and on Agreeableness, i.e. scoring higher than 40 on each of these two categories.
traits %>% 
  filter(Extroversion > 40 & Agreeableness > 40)
```

## Factorize, round two

In the last exercise you learned how to import a data file using the command `read_sav()`. With SPSS data files, it can also happen that some of the variables you import have the `labelled` class. This is done to keep all the labelling information that was originally present in the `.sav` and `.por` files. It's advised to coerce (or change) these variables to factors or other standard R classes.

The data for this exercise involves information on employees and their demographic and economic attributes (Source: QRiE). The data can be found on the following URL:

http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav

```{r}
# haven is already loaded

# Import SPSS data from the URL: work
work <- read_sav("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav")

# Display summary of work$GENDER
summary(work$GENDER)


# Convert work$GENDER to a factor
work$GENDER <- as_factor(work$GENDER)


# Display summary of work$GENDER again
summary(work$GENDER)

```

## Import STATA data with foreign (1)

The foreign package offers a simple function to import and read STATA data: `read.dta()`.

In this exercise, you will import data on the US presidential elections in the year 2000. The data in `florida.dta` contains the total numbers of votes for each of the four candidates as well as the total number of votes per election area in the state of Florida (Source: Florida Department of State). The file is available in your working directory.

```{r}
# Load the foreign package
# foreign is already loaded

# Import florida.dta and name the resulting data frame florida
florida <- read.dta("florida.dta")

# Check tail() of florida
tail(florida)
```

## Import STATA data with foreign (2)

Data can be very diverse, going from character vectors to categorical variables, dates and more. It's in these cases that the additional arguments of `read.dta()` will come in handy.

The arguments you will use most often are `convert.dates`, `convert.factors`, `missing.type` and `convert.underscore`. Their meaning is pretty straightforward, as Filip explained in the video. It's all about correctly converting STATA data to standard R data structures. Type `?read.dta` to find out about about the default values.

The dataset for this exercise contains socio-economic measures and access to education for different individuals (Source: World Bank). This data is available as `edequality.dta`, which is located in the worldbank folder in your working directory.

**Instructions**

- Specify the path to the file using `file.path()`. Call it `path`. Remember the `"edequality.dta"` file is located in the `"worldbank"` folder.
- Use the `path` variable to import the data file in three different ways; each time show its structure with `str()`:
- `edu_equal_1`: By passing only the file `path` to `read.dta()`.
- `edu_equal_2`: By passing the file `path`, and setting `convert.factors` to `FALSE`.
- `edu_equal_3`: By passing the file `path`, and setting `convert.underscore` to `TRUE`.

```{r}
# Read and print the structure of edequality.dta: edu_equal_1
edu_equal_1 <- read.dta("edequality.dta")
str(edu_equal_1)

# edu_equal_2
edu_equal_2 <- read.dta("edequality.dta", convert.factors = FALSE)
str(edu_equal_2)

# edu_equal_3
edu_equal_3 <- read.dta("edequality.dta", convert.underscore = TRUE)
str(edu_equal_3)
```

```{r}
#Using edu_equal_1 answer the following question
# How many observations/individuals of Bulgarian ethnicity have an income above 1000?
edu_equal_1 %>% 
  filter(ethnicity_head == "Bulgaria" & income > 1000) %>% 
  count()

nrow(subset(edu_equal_1,ethnicity_head == "Bulgaria" & income > 1000))
```

## Import SPSS data with foreign (1)

All great things come in pairs. Where `foreign` provided `read.dta()` to read Stata data, there's also `read.spss()` to read SPSS data files. To get a data frame, make sure to set `to.data.frame = TRUE` inside `read.spss()`.

In this exercise, you'll be working with socio-economic variables from different countries (Source: Quantative Data Analysis in Education). The SPSS data is in a file called `international.sav`, which is in your working directory. 

```{r}
# foreign is already loaded

# Import international.sav as a data frame: demo
demo <- read.spss("international.sav", to.data.frame = TRUE)

# Create boxplot of gdp variable of demo
boxplot(demo$gdp)

```

## Import SPSS data with foreign (2)

In the previous exercise, you used the `to.data.frame` argument inside `read.spss()`. There are many other ways in which to customize the way your SPSS data is imported.

In this exercise you will experiment with another argument, `use.value.labels`. It specifies whether variables with value labels should be converted into R factors with levels that are named accordingly. The argument is `TRUE` by default which means that so called labelled variables inside SPSS are converted to factors inside R.

You'll again be working with the `international.sav` data, which is available in your current working directory.

```{r}
# foreign is already loaded

# Import international.sav as demo_1
demo_1 <- read.spss("international.sav", to.data.frame = TRUE)

# Print out the head of demo_1
head(demo_1)

# Import international.sav as demo_2
demo_2 <- read.spss("international.sav",to.data.frame = TRUE, use.value.labels = FALSE)

# print head of demo_2
head(demo_2)
```

